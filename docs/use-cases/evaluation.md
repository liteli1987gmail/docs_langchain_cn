# 评估的方法 Evaluation

这部分文档涵盖了我们在 LangChain 中对评估的处理方式和思考方式，包括对内部链式 操作/代理 的评估，以及我们建议构建在 LangChain 之上的人如何进行评估的方法。

## 问题
评估 LangChain 链式操作和代理可能非常困难。这主要有两个原因：

1. 缺乏数据

在开始项目之前，通常很难获得用于评估链式操作和代理的大量数据。这通常是因为大型语言模型（大多数链式操作和代理的核心）具有出色的小样本和零样本学习能力，这意味着您几乎总是能够在特定任务（如文本到 SQL、问答等）上开始，而无需大量的示例数据集。这与传统的机器学习形成鲜明对比，传统机器学习在使用模型之前必须先收集大量数据点。

2. 缺乏评估指标

大多数链式操作和代理执行的任务往往没有很好的评估指标来评估性能。例如，生成某种形式的文本比评估分类预测或数值预测要复杂得多。

## 解决方案
LangChain 试图解决这两个问题。我们目前提供的解决方案是初步的尝试，我们并不认为这是完美的解决方案。因此，我们非常欢迎关于此问题的反馈、贡献、整合和想法。

目前我们针对每个问题的解决方案如下：

1. 缺乏数据

我们已经创建了 LangChainDatasets，这是 Hugging Face 上的一个社区空间。我们希望它成为一个用于评估常见链式操作和代理的开源数据集的集合。我们已经贡献了五个自己的数据集作为起点，但我们非常希望这成为一个社区的努力。要贡献数据集，您只需要加入社区，然后您就可以上传数据集。

我们还致力于尽可能简化人们创建自己数据集的过程。作为初步尝试，我们添加了一个名为 QAGenerationChain 的链式操作，它可以根据文档生成问题-答案对，用于以后评估问题回答任务。

2. 缺乏评估指标

针对缺乏评估指标，我们有两个解决方案。

第一个解决方案是不使用评估指标，而是依靠目测结果来对链式操作和代理的性能有所了解。为了辅助这一点，我们开发了（并将继续开发）Tracing，这是一个基于 UI 的可视化工具，可以查看链式操作和代理运行的结果。

第二个解决方案是使用语言模型本身来评估输出结果。为此，我们提供了一些针对此问题的链式操作和提示。

请注意，这些解决方案仍然在不断发展和改进中。我们欢迎用户提供反馈和建议，以帮助我们改进 LangChain 的评估功能。我们希望通过不断的改进和开放性的合作，为 LangChain 用户提供更好的评估工具和方法。